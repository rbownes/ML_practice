{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression from scratch\n",
    "## Pytorch and Autograd basics\n",
    "\n",
    "This notebook is going to go through a first principals approach to creating a linear regression model using Pytorch and its built in autodif library, Autograd. We are only going to need two libraries for this, Numpy and Torch. Numpy is specifically just for creating tensors in familiar ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors, N dimensional arrays and linear equations. \n",
    "\n",
    "The illustration below highlights this really nicely, where by there is increased dimensionality from a vector, a 1-D structure, to a matrix, a 2-D structure and tensors, which are N-Dimensional and can also be interpreted as stacked matrices.\n",
    "\n",
    "<img src=\"scalar-vector-matrix-tensor.jpg\">\n",
    "\n",
    "The first thing we are going to do is instantiate some tensors with the values to solve the linear equation below:\n",
    "\n",
    "$y = W * X + B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., requires_grad=True)\n",
      "tensor(2., requires_grad=True)\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3.)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created the W X and B variables, and declared ```requires_grad=True``` for X and W. This creates a computational graph for W and X which tracks the operation history for these varialbes and forms a backward graph for gradient calculation.\n",
    "\n",
    "<img src=\"grad_graph.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "y = w * x + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far all we have done is solve the linear combination of W, X and B for y which is 5. Now we can illustrate what we are using the autodiff capabilities of pytorch/autograd for. Given that we know the product of these tensors is 5, we know that the product of W or X and it's gradient, the slope of the equation at that point, plus the intercept B, will give us 5. We can prove this below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check: x = 1, it's gradient is 2 so X * X.grad + B = 5. Keep this in mind, this will come in handy later, when we are using this feature to minimize a loss function to converge a very simple linear regression model, and because we are still in lockdown we are going to try and fit the flavour (subjective) and degree of crustiness (also subjective) of my home made sour dough to the measured proofing time, acidity and age of my starter.\n",
    "\n",
    "## Linear Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.   4.5 32. ]\n",
      " [15.   4.2 40. ]\n",
      " [18.   4.2 45. ]\n",
      " [24.   4.  51. ]\n",
      " [30.   3.8 58. ]\n",
      " [18.   3.8 65. ]\n",
      " [15.   4.  70. ]\n",
      " [15.   4.1 78. ]\n",
      " [18.   3.9 81. ]\n",
      " [24.   4.  90. ]]\n"
     ]
    }
   ],
   "source": [
    "# Input (Proofing time, acidity, age of starter)\n",
    "inputs = np.array([[12, 4.5, 32], \n",
    "                   [15, 4.2, 40], \n",
    "                   [18, 4.2, 45], \n",
    "                   [24, 4.0, 51], \n",
    "                   [30, 3.8, 58],\n",
    "                   [18, 3.8, 65],\n",
    "                   [15, 4.0, 70],\n",
    "                   [15, 4.1, 78],\n",
    "                   [18, 3.9, 81],\n",
    "                   [24, 4.0, 90]], dtype='float32')\n",
    "# Targets (Flavour, Crust)\n",
    "targets = np.array([[70, 70], \n",
    "                    [72, 75], \n",
    "                    [80, 70], \n",
    "                    [85, 75], \n",
    "                    [85, 80],\n",
    "                    [82, 85], \n",
    "                    [90, 85], \n",
    "                    [87, 90], \n",
    "                    [85, 90], \n",
    "                    [90, 95]], dtype='float32')\n",
    "print(inputs) # This is an Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.0000,  4.5000, 32.0000],\n",
      "        [15.0000,  4.2000, 40.0000],\n",
      "        [18.0000,  4.2000, 45.0000],\n",
      "        [24.0000,  4.0000, 51.0000],\n",
      "        [30.0000,  3.8000, 58.0000],\n",
      "        [18.0000,  3.8000, 65.0000],\n",
      "        [15.0000,  4.0000, 70.0000],\n",
      "        [15.0000,  4.1000, 78.0000],\n",
      "        [18.0000,  3.9000, 81.0000],\n",
      "        [24.0000,  4.0000, 90.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs) #Â we are using the from_numpy method which doesn't copy the data but provides a reference.\n",
    "targets = torch.from_numpy(targets) # This is an efficient method when the data is large, probably unnecessary for data of this size.\n",
    "print(inputs) # This is the converted Tensor where the type has been converted to float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we now have a matrix of 10x3 inputs, and a matrix of 10x2 targets.\n",
    "\n",
    "In an ideal situation, we would multiply our inputs, X, by some weights w, and add a bias, b. This will return our target values of y for the correct values of w and b. Now, we are unlikely to guess the correct values of w and b on the first guess, but there are a few ways forward. We could grid or random search a matrix of values to brute force this, but this is not a very sophisticated method and will not necessarily find the minima for latent space if that area is outside the defined search area. \n",
    "\n",
    "$$\\begin{bmatrix} y1 & y1 \\\\ y2 & y2 \\\\ y3 & y3 \\end{bmatrix} = \\begin{bmatrix} x1 & x1 & x1 \\\\ x2 & x2 & x2 \\\\ x3 & x3 & x3 \\end{bmatrix}\\begin{bmatrix} w1 & w1 \\\\ w2 & w2 \\\\ w3 & w3 \\end{bmatrix} + \\begin{bmatrix} b1 & b1 \\\\ b2 & b2 \\\\ b3 & b3 \\end{bmatrix}$$\n",
    "\n",
    "So we will first create random weights and biases, requiring grad for autodifferentiation. The weights are in the shape 2x3, we will multiply the inputs by the transpose of this matrix to obtain the desired shape of the target tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5543, -0.6560,  1.4981],\n",
      "        [ 0.0123,  1.5953, -0.6069]], requires_grad=True)\n",
      "tensor([-0.9139,  0.7198], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model is just the formalisation of the y = mx + b above, with the matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a single example of how this equation solves for y for a single, random, set of weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 50.7237, -11.3739],\n",
       "        [ 64.5679, -16.6706],\n",
       "        [ 73.7212, -19.6681],\n",
       "        [ 86.1666, -23.5547],\n",
       "        [100.1101, -28.0481],\n",
       "        [103.9450, -32.4437],\n",
       "        [109.6413, -35.1959],\n",
       "        [121.5603, -39.8913],\n",
       "        [127.8486, -41.9941],\n",
       "        [144.5914, -47.2227]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 50.7237, -11.3739],\n",
      "        [ 64.5679, -16.6706],\n",
      "        [ 73.7212, -19.6681],\n",
      "        [ 86.1666, -23.5547],\n",
      "        [100.1101, -28.0481],\n",
      "        [103.9450, -32.4437],\n",
      "        [109.6413, -35.1959],\n",
      "        [121.5603, -39.8913],\n",
      "        [127.8486, -41.9941],\n",
      "        [144.5914, -47.2227]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[70., 70.],\n",
      "        [72., 75.],\n",
      "        [80., 70.],\n",
      "        [85., 75.],\n",
      "        [85., 80.],\n",
      "        [82., 85.],\n",
      "        [90., 85.],\n",
      "        [87., 90.],\n",
      "        [85., 90.],\n",
      "        [90., 95.]])\n"
     ]
    }
   ],
   "source": [
    "# Compare with targets\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above two cells show the calculated predictions for y and the targets we are aiming for. They are not very close, and it is pretty easy to calculate how close! \n",
    "\n",
    "To do that we will use mean square error, MSE, which finds the average squared error between two tensors. In this case, the difference between the predicted values and the target values is squared, the tensor summed, and then divided by the number of elements in the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6739.4248, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are now going to use the features of autograd on the loss product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the components of the computational graph that make up loss that require autograd in their instantiation calling backward on the object loss will automatically calculate the gradient for these components, in this case, w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5543, -0.6560,  1.4981],\n",
      "        [ 0.0123,  1.5953, -0.6069]], requires_grad=True)\n",
      "tensor([[  331.4986,    60.6884,  1362.0184],\n",
      "        [-2122.6865,  -447.5899, -7126.9585]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9139,  0.7198], requires_grad=True)\n",
      "tensor([  15.6876, -111.1063])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what the gradient tells us, in practical terms, is in which direction the nearest function minima is. If the gradient is positive, reducing the value will step the function back toward the minima, if the gradient is negative making it larger, will bring it closer to that minima. So if we adjust the weights and biases in the correct direction, for every step in a training loop we will coninually move the values off the weights and biases toward the optimal values, and reduce the loss as much as we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "for i in range(100000): # do this 100,000 times\n",
    "    preds = model(inputs) # predict the targets for the input values\n",
    "    loss = mse(preds, targets) # calculate the loss\n",
    "    loss.backward() # calculate the gradients\n",
    "    with torch.no_grad(): # the weight update step shouldn't be added to the computational graph.\n",
    "        w -= w.grad * 1e-5 # incriment the weights by a tiny percentage of the gradient\n",
    "        b -= b.grad * 1e-5 # incriment the bias by a tiny percentage of the gradient\n",
    "        w.grad.zero_() # clear the gradients\n",
    "        b.grad.zero_() # clear the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final weights and biases for this model are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0813, 7.9036, 0.4634],\n",
      "        [0.4770, 8.0436, 0.6078]], requires_grad=True)\n",
      "tensor([0.9097, 2.1006], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a final loss of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.5815, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's compare the predicted final values with the actual targets to see how well we have model the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 64.2809,  63.4700],\n",
       "        [ 68.8611,  67.3503],\n",
       "        [ 74.4221,  71.8202],\n",
       "        [ 82.1099,  76.7202],\n",
       "        [ 90.2610,  82.2280],\n",
       "        [ 80.5289,  80.7585],\n",
       "        [ 81.1827,  83.9751],\n",
       "        [ 85.6803,  89.6418],\n",
       "        [ 88.7338,  91.2874],\n",
       "        [100.1829, 100.4239]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[70., 70.],\n",
       "        [72., 75.],\n",
       "        [80., 70.],\n",
       "        [85., 75.],\n",
       "        [85., 80.],\n",
       "        [82., 85.],\n",
       "        [90., 85.],\n",
       "        [87., 90.],\n",
       "        [85., 90.],\n",
       "        [90., 95.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly! It is 100% possible to easily commute this to a GPU by moving the tensors and model to a gpu by calling device and pointing to the required bit of hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = torch.from_numpy(inputs).to(device)\n",
    "targets = torch.from_numpy(targets).to(device)\n",
    "\n",
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs).to(device)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the NN built in methods of pytorch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 231.43313598632812\n",
      "epoch 200, loss 192.86708068847656\n",
      "epoch 300, loss 173.49008178710938\n",
      "epoch 400, loss 162.86288452148438\n",
      "epoch 500, loss 156.2476348876953\n",
      "epoch 600, loss 151.49337768554688\n",
      "epoch 700, loss 147.62332153320312\n",
      "epoch 800, loss 144.19320678710938\n",
      "epoch 900, loss 141.00059509277344\n",
      "epoch 1000, loss 137.9528045654297\n",
      "epoch 1100, loss 135.00689697265625\n",
      "epoch 1200, loss 132.14260864257812\n",
      "epoch 1300, loss 129.34991455078125\n",
      "epoch 1400, loss 126.62345886230469\n",
      "epoch 1500, loss 123.96012878417969\n",
      "epoch 1600, loss 121.35768127441406\n",
      "epoch 1700, loss 118.81441497802734\n",
      "epoch 1800, loss 116.328857421875\n",
      "epoch 1900, loss 113.89957427978516\n",
      "epoch 2000, loss 111.52531433105469\n",
      "epoch 2100, loss 109.20477294921875\n",
      "epoch 2200, loss 106.93680572509766\n",
      "epoch 2300, loss 104.7201156616211\n",
      "epoch 2400, loss 102.55360412597656\n",
      "epoch 2500, loss 100.43608093261719\n",
      "epoch 2600, loss 98.36651611328125\n",
      "epoch 2700, loss 96.34379577636719\n",
      "epoch 2800, loss 94.36680603027344\n",
      "epoch 2900, loss 92.43456268310547\n",
      "epoch 3000, loss 90.5460433959961\n",
      "epoch 3100, loss 88.70028686523438\n",
      "epoch 3200, loss 86.89627838134766\n",
      "epoch 3300, loss 85.13311004638672\n",
      "epoch 3400, loss 83.4097900390625\n",
      "epoch 3500, loss 81.72550201416016\n",
      "epoch 3600, loss 80.07939147949219\n",
      "epoch 3700, loss 78.47044372558594\n",
      "epoch 3800, loss 76.89793395996094\n",
      "epoch 3900, loss 75.36097717285156\n",
      "epoch 4000, loss 73.85881805419922\n",
      "epoch 4100, loss 72.39070129394531\n",
      "epoch 4200, loss 70.95575714111328\n",
      "epoch 4300, loss 69.55333709716797\n",
      "epoch 4400, loss 68.1825942993164\n",
      "epoch 4500, loss 66.84288787841797\n",
      "epoch 4600, loss 65.5334701538086\n",
      "epoch 4700, loss 64.25371551513672\n",
      "epoch 4800, loss 63.0029182434082\n",
      "epoch 4900, loss 61.780426025390625\n",
      "epoch 5000, loss 60.58557891845703\n",
      "epoch 5100, loss 59.417808532714844\n",
      "epoch 5200, loss 58.27643585205078\n",
      "epoch 5300, loss 57.16088104248047\n",
      "epoch 5400, loss 56.070594787597656\n",
      "epoch 5500, loss 55.0050048828125\n",
      "epoch 5600, loss 53.96348190307617\n",
      "epoch 5700, loss 52.9455451965332\n",
      "epoch 5800, loss 51.950645446777344\n",
      "epoch 5900, loss 50.978233337402344\n",
      "epoch 6000, loss 50.02783966064453\n",
      "epoch 6100, loss 49.09895706176758\n",
      "epoch 6200, loss 48.19108963012695\n",
      "epoch 6300, loss 47.303768157958984\n",
      "epoch 6400, loss 46.436546325683594\n",
      "epoch 6500, loss 45.588924407958984\n",
      "epoch 6600, loss 44.760475158691406\n",
      "epoch 6700, loss 43.950782775878906\n",
      "epoch 6800, loss 43.15941619873047\n",
      "epoch 6900, loss 42.385963439941406\n",
      "epoch 7000, loss 41.63001251220703\n",
      "epoch 7100, loss 40.891143798828125\n",
      "epoch 7200, loss 40.16905212402344\n",
      "epoch 7300, loss 39.463233947753906\n",
      "epoch 7400, loss 38.773414611816406\n",
      "epoch 7500, loss 38.09921646118164\n",
      "epoch 7600, loss 37.44024658203125\n",
      "epoch 7700, loss 36.79621887207031\n",
      "epoch 7800, loss 36.166725158691406\n",
      "epoch 7900, loss 35.55152130126953\n",
      "epoch 8000, loss 34.950191497802734\n",
      "epoch 8100, loss 34.36249542236328\n",
      "epoch 8200, loss 33.78809356689453\n",
      "epoch 8300, loss 33.2266845703125\n",
      "epoch 8400, loss 32.6779899597168\n",
      "epoch 8500, loss 32.141700744628906\n",
      "epoch 8600, loss 31.6175537109375\n",
      "epoch 8700, loss 31.10526466369629\n",
      "epoch 8800, loss 30.604583740234375\n",
      "epoch 8900, loss 30.115222930908203\n",
      "epoch 9000, loss 29.636917114257812\n",
      "epoch 9100, loss 29.169448852539062\n",
      "epoch 9200, loss 28.71256446838379\n",
      "epoch 9300, loss 28.26601219177246\n",
      "epoch 9400, loss 27.829565048217773\n",
      "epoch 9500, loss 27.40300941467285\n",
      "epoch 9600, loss 26.986072540283203\n",
      "epoch 9700, loss 26.578594207763672\n",
      "epoch 9800, loss 26.18032455444336\n",
      "epoch 9900, loss 25.79107666015625\n",
      "epoch 10000, loss 25.410633087158203\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # Convert numpy array to torch Variable\n",
    "    values = inputs.clone().detach().requires_grad_(True)\n",
    "    labels = targets.clone().detach().requires_grad_(True)\n",
    "    #inputs = inputs.clone().detach().to(device)\n",
    "    #labels = targets.clone().detach().to(device)\n",
    "\n",
    "    # Clear gradients w.r.t. parameters\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    # Forward to get output\n",
    "    outputs = model(values)\n",
    "\n",
    "    # Calculate Loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Getting gradients w.r.t. parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchlight",
   "language": "python",
   "name": "torchlight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
